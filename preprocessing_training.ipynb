{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Mesothelioma Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('meso.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)): \n",
    "    if Y[i] == 2:\n",
    "        Y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47.  1.  0. ...  0.  0. 34.]\n",
      " [55.  1.  0. ...  1.  1. 42.]\n",
      " [29.  1.  1. ...  0.  0. 43.]\n",
      " ...\n",
      " [58.  1.  6. ...  0.  1. 68.]\n",
      " [42.  1.  6. ...  1.  0. 78.]\n",
      " [54.  1.  0. ...  1.  0. 45.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25423309 -1.20686442 -1.03063205 ...  0.81348922 -1.04527154\n",
      "   0.61040794]\n",
      " [-0.75217371 -1.20686442 -0.20758313 ...  0.81348922  0.95668921\n",
      "   0.52421475]\n",
      " [-2.94797037 -1.20686442 -1.03063205 ... -1.22927259 -1.04527154\n",
      "  -0.42391029]\n",
      " ...\n",
      " [ 0.71169073 -1.20686442  1.43851469 ...  0.81348922  0.95668921\n",
      "   0.61040794]\n",
      " [-0.02024149 -1.20686442 -0.61910759 ...  0.81348922  0.95668921\n",
      "   0.61040794]\n",
      " [ 0.80318226  0.82859349 -1.03063205 ...  0.81348922 -1.04527154\n",
      "   0.99827727]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing PCA on dataset for Dimentionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25216517 -1.10819812  0.94878094 ...  2.32792395  0.74974737\n",
      "  -1.34555446]\n",
      " [-1.27378499 -0.39779663 -0.66878045 ...  0.82798641  2.13157134\n",
      "  -0.04178804]\n",
      " [ 1.09788884  0.36881018  0.34507897 ... -0.94692524  1.13885766\n",
      "  -0.21527782]\n",
      " ...\n",
      " [ 0.12239457  2.01161015  1.23288378 ... -0.26121869  0.26319944\n",
      "   1.25907715]\n",
      " [-1.31322545 -0.40886626  0.56887131 ... -0.42303245  0.03668646\n",
      "   0.78021005]\n",
      " [ 0.12470052 -0.30633533 -1.66863637 ... -1.33504749  0.10142783\n",
      "   0.71513577]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 20)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(X_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +-------------------------------------------------------------------+\n",
    "# |  Author: Siddhant Sudesh Chalke, Roll No. 21BCS118, IIIT Dharwad  |\n",
    "# +-------------------------------------------------------------------+\n",
    "\n",
    "# Using Gradient Boosting for predicting Mesothelioma\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth = None):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def get_entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts = True)\n",
    "        probablities = counts / len(y)\n",
    "        entropy = -np.sum(probablities * np.log2(probablities))\n",
    "\n",
    "        return entropy\n",
    "    \n",
    "    # Splitting data based on feature and threshold\n",
    "    def split_data(self, x, y, feature_index, threshold):\n",
    "        left = x[:, feature_index] <= threshold\n",
    "        right = ~left\n",
    "\n",
    "        return x[left], x[right], y[left], y[right]\n",
    "    \n",
    "    # Finding the best feature and threshold to split the data\n",
    "    def get_best_split(self, x, y):\n",
    "        best_gain = 0\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        n_samples, n_features = x.shape\n",
    "        base_entropy = self.get_entropy(y)\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(x[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                x_left, x_right, y_left, y_right = self.split_data(x, y, feature_index, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                left_entropy = self.get_entropy(y_left)\n",
    "                right_entropy = self.get_entropy(y_right)\n",
    "\n",
    "                information_gain = base_entropy - ((len(y_left) / n_samples) * left_entropy + (len(y_right) / n_samples) * right_entropy)\n",
    "\n",
    "                if information_gain > best_gain:\n",
    "                    best_gain = information_gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "    \n",
    "    # Recursively building the decision tree\n",
    "    def build_tree(self, x, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        best_feature_index, best_threshold = self.get_best_split(x, y)\n",
    "        if best_feature_index is None:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        x_left, x_right, y_left, y_right = self.split_data(x, y, best_feature_index, best_threshold)\n",
    "\n",
    "        left_subtree = self.build_tree(x_left, y_left, depth + 1)\n",
    "        right_subtree = self. build_tree(x_right, y_right, depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature_index' : best_feature_index,\n",
    "            'threshold' : best_threshold,\n",
    "            'left' : left_subtree,\n",
    "            'right' : right_subtree\n",
    "        }\n",
    "    \n",
    "    # Fitting the decision tree to the data\n",
    "    def fit(self, x, y):\n",
    "        self.tree = self.build_tree(x, y, depth = 0)\n",
    "\n",
    "    # Predicting for a single datapoint\n",
    "    def single_prediction(self, sample, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if sample[tree['feature_index']] <= tree['threshold']:\n",
    "                return self.single_prediction(sample, tree['left'])\n",
    "            else:\n",
    "                return self.single_prediction(sample, tree['right'])\n",
    "        else:\n",
    "            return tree\n",
    "        \n",
    "    # Predicting class of multiple samples\n",
    "    def predict(self, x):\n",
    "        return np.array([self.single_prediction(sample, self.tree) for sample in x])\n",
    "\n",
    "\n",
    "# Gradient Boosting\n",
    "class GradientBoostingClassifer:\n",
    "    def __init__(self, n_estimators = 50, learning_rate = 0.01, max_depth = 3, threshold_probability = 0.75):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.threshold_probability = threshold_probability\n",
    "        self.estimators = []\n",
    "    \n",
    "    # Function to return all the parameters of the Gradient Boosting Classifier\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth,\n",
    "            'threshold_probability': self.threshold_probability\n",
    "        }\n",
    "\n",
    "    # Function to set parameters of the Gradient Boosting Classifier\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "    # Sigmoid function to get probablity from numerical predictions\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def residuals(self, y, y_pred):\n",
    "        return y - self.sigmoid(y_pred)\n",
    "    \n",
    "    # Fitting Gradient boosting classifier to data\n",
    "    def fit(self, x, y):\n",
    "        n_samples = len(x)\n",
    "        y_pred = np.zeros(n_samples)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            residual = self.residuals(y, y_pred)\n",
    "            tree = DecisionTree(max_depth = self.max_depth)\n",
    "            tree.fit(x, residual)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "            y_pred = y_pred + self.learning_rate * tree.predict(x)\n",
    "    \n",
    "    # Predict function to classify based on given features\n",
    "    def predict(self, x):\n",
    "        y_pred = np.zeros(len(x))\n",
    "        for tree in self.estimators:\n",
    "            y_pred = y_pred + self.learning_rate * tree.predict(x)\n",
    "        \n",
    "        y_res = []\n",
    "        for y in self.sigmoid(y_pred):\n",
    "            if y >= self.threshold_probability:\n",
    "                y_res.append(1)\n",
    "            else:\n",
    "                y_res.append(0)\n",
    "        \n",
    "        return y_res\n",
    "    \n",
    "    # Function to return accuracy score of the the Gradient Boosting Classifier\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 350, 400, 450, 500],\n",
    "    'max_depth': [50, 50, 40, 40, 40, 30, 30]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_clf = GradientBoostingClassifer()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'n_estimators': 350}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tree_classifier = GradientBoostingClassifer(n_estimators = 350, learning_rate = 0.01, max_depth = 50, threshold_probability = 0.7)\n",
    "tree_classifier.fit(X_train_pca, Y_train)\n",
    "\n",
    "y_pred = tree_classifier.predict(X_test_pca)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 74.49%\n",
      "Hamming Loss: 0.2551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "\n",
    "print(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_true = Y_test, y_pred = y_pred) * 100))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_loss(y_true = Y_test, y_pred = y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
